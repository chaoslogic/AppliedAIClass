{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 12: Training a Perceptron to Implement XOR Gate .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fuzMFUW0L-O"
      },
      "source": [
        "# **Training a Perceptron to Implement an XOR Gate**\n",
        "\n",
        "\n",
        "## PY599 (Fall 2018): Applied Artificial Intelligence\n",
        "## NC State University\n",
        "###Dr. Behnam Kia\n",
        "### https://appliedai.wordpress.ncsu.edu/\n",
        "\n",
        "\n",
        "**Disclaimer**: Please note that these codes are simplified version of the algorithms, and they may not give the best, or expected performance that you could possibly get from these algorithms. The aim of this notebook is to help you understand the basics and the essence of these algorithms, and experiment with them. These basic codes are not deployment-ready or free-of-errors for real-world applications. To learn more about these algorithms please refer to text books that specifically study these algorithms, or contact me. - Behnam Kia\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X8GXl9W0VfM"
      },
      "source": [
        "## Synthetic Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vglu5TFu0LL4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "8dabce33-ee7f-45e6-85af-c92420aa249f"
      },
      "source": [
        "import numpy as np                   #this is our old friend, the handy NumPy\n",
        "import matplotlib.pyplot as plt      # matplotlib takes cares of ploting   \n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "\n",
        "def XOR(a,b):\n",
        "  return np.where(a != b, 1, 0)\n",
        "\n",
        "      \n",
        "training_size=4      \n",
        "# we create a synthetic data set      \n",
        "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_train = np.zeros((1,4))\n",
        "\n",
        "y_train=XOR(x_train[:,0],x_train[:,1])\n",
        "\n",
        "\n",
        "print(y_train)\n",
        "\n",
        "for xx in x_train:\n",
        "  if (XOR(xx[0],xx[1])==1):\n",
        "    ax.plot(xx[0],xx[1],'b.')\n",
        "  else:\n",
        "    ax.plot(xx[0],xx[1],'r.')\n",
        "\n",
        "plt.xlabel(\"a\", fontsize=25)\n",
        "plt.ylabel(\"b\", fontsize=25)\n",
        "plt.title('XOR function. Reds are zero, and blues are 1')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 1 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'XOR function. Reds are zero, and blues are 1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAF1CAYAAADiLzM9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9918ab5c88>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrkVpV59IoWN"
      },
      "source": [
        "def LogisticRegression(W,x):\n",
        "  y = x.dot(W)\n",
        "  return sigmoid(y)\n",
        "\n",
        "def sigmoid(y):\n",
        "  return 1/(1+np.exp(-y))\n",
        "\n",
        "def DecisionBoundary(p):\n",
        "  k=len(p)\n",
        "  result=np.zeros((k,1))\n",
        "  for i in range(k):\n",
        "    if(p[i]>0.5):\n",
        "      result[i]=1\n",
        "    else:\n",
        "      result[i]=0\n",
        "  return result    \n",
        "\n",
        "  \n",
        "def CrossEntropy(y_hat,y):\n",
        "  \n",
        "  return 1/np.size(y_hat)*(y.T.dot(np.log(y_hat)) - (1-y).T.dot((np.log(1-y_hat))))\n",
        "  \n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwjn_RyWj5Ug"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RugfGZe-Z82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3537
        },
        "outputId": "b0edf2f9-0c5a-44c3-ebf0-1d2041d79668"
      },
      "source": [
        "\n",
        "from mpl_toolkits import mplot3d\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_trainp=np.c_[np.ones((int(training_size))),x_train]  #padding x_train with ones\n",
        "\n",
        "y_train = np.reshape(y_train,(np.size(y_train),1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "alpha = 2\n",
        "n_iterations = 100\n",
        "m = training_size  \n",
        "\n",
        "\n",
        "np.random.seed(seed=80)\n",
        "W = 5*np.random.randn(3,1) # we start from a randomly initialzied set of parameters for Logistic Regression\n",
        "\n",
        "\n",
        "Wtrajectory=np.array(W)\n",
        "\n",
        "\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 1/m * x_trainp.T.dot(sigmoid(x_trainp.dot(W)) - y_train) #this is pretty much similiar to gradient in Linear Regression, but y_hat has a different definition.\n",
        "    W = W - alpha * gradients   # learning algorithm, we follow the gradient. \n",
        "    Wtrajectory=np.hstack((Wtrajectory,W))\n",
        "    y_hat = LogisticRegression(W,x_trainp)\n",
        "    CE = CrossEntropy(y_hat,y_train)\n",
        "    Binary_result=DecisionBoundary(y_hat)\n",
        "    num_correct_predition=np.sum(y_train == Binary_result)\n",
        "\n",
        "  \n",
        "    print(\"Epoch %d/%d\"%(iteration+1,n_iterations))  #here we are printing readable outputs to track the performance of the optimization as GD iterates\n",
        "    print(\"loss:  %s - acc: %s\"%(CE,num_correct_predition/len(y_train))) \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "loss:  [[5.37969986]] - acc: 0.5\n",
            "Epoch 2/100\n",
            "loss:  [[4.62978243]] - acc: 0.5\n",
            "Epoch 3/100\n",
            "loss:  [[3.87992253]] - acc: 0.5\n",
            "Epoch 4/100\n",
            "loss:  [[3.1299998]] - acc: 0.5\n",
            "Epoch 5/100\n",
            "loss:  [[2.37952763]] - acc: 0.5\n",
            "Epoch 6/100\n",
            "loss:  [[1.63160625]] - acc: 0.5\n",
            "Epoch 7/100\n",
            "loss:  [[0.93762517]] - acc: 0.5\n",
            "Epoch 8/100\n",
            "loss:  [[0.46686018]] - acc: 0.25\n",
            "Epoch 9/100\n",
            "loss:  [[0.25820921]] - acc: 0.5\n",
            "Epoch 10/100\n",
            "loss:  [[0.17143844]] - acc: 0.5\n",
            "Epoch 11/100\n",
            "loss:  [[0.13190485]] - acc: 0.5\n",
            "Epoch 12/100\n",
            "loss:  [[0.11205397]] - acc: 0.5\n",
            "Epoch 13/100\n",
            "loss:  [[0.10051309]] - acc: 0.5\n",
            "Epoch 14/100\n",
            "loss:  [[0.09240127]] - acc: 0.5\n",
            "Epoch 15/100\n",
            "loss:  [[0.08569069]] - acc: 0.5\n",
            "Epoch 16/100\n",
            "loss:  [[0.07959774]] - acc: 0.5\n",
            "Epoch 17/100\n",
            "loss:  [[0.0738521]] - acc: 0.5\n",
            "Epoch 18/100\n",
            "loss:  [[0.06837959]] - acc: 0.5\n",
            "Epoch 19/100\n",
            "loss:  [[0.06317328]] - acc: 0.5\n",
            "Epoch 20/100\n",
            "loss:  [[0.05824441]] - acc: 0.5\n",
            "Epoch 21/100\n",
            "loss:  [[0.05360487]] - acc: 0.5\n",
            "Epoch 22/100\n",
            "loss:  [[0.04926163]] - acc: 0.5\n",
            "Epoch 23/100\n",
            "loss:  [[0.04521571]] - acc: 0.5\n",
            "Epoch 24/100\n",
            "loss:  [[0.04146265]] - acc: 0.5\n",
            "Epoch 25/100\n",
            "loss:  [[0.0379936]] - acc: 0.5\n",
            "Epoch 26/100\n",
            "loss:  [[0.03479645]] - acc: 0.5\n",
            "Epoch 27/100\n",
            "loss:  [[0.0318569]] - acc: 0.5\n",
            "Epoch 28/100\n",
            "loss:  [[0.02915931]] - acc: 0.5\n",
            "Epoch 29/100\n",
            "loss:  [[0.02668744]] - acc: 0.5\n",
            "Epoch 30/100\n",
            "loss:  [[0.02442498]] - acc: 0.5\n",
            "Epoch 31/100\n",
            "loss:  [[0.02235591]] - acc: 0.5\n",
            "Epoch 32/100\n",
            "loss:  [[0.02046482]] - acc: 0.5\n",
            "Epoch 33/100\n",
            "loss:  [[0.01873706]] - acc: 0.5\n",
            "Epoch 34/100\n",
            "loss:  [[0.01715886]] - acc: 0.5\n",
            "Epoch 35/100\n",
            "loss:  [[0.01571739]] - acc: 0.5\n",
            "Epoch 36/100\n",
            "loss:  [[0.01440075]] - acc: 0.5\n",
            "Epoch 37/100\n",
            "loss:  [[0.01319798]] - acc: 0.5\n",
            "Epoch 38/100\n",
            "loss:  [[0.01209902]] - acc: 0.5\n",
            "Epoch 39/100\n",
            "loss:  [[0.01109466]] - acc: 0.5\n",
            "Epoch 40/100\n",
            "loss:  [[0.01017648]] - acc: 0.5\n",
            "Epoch 41/100\n",
            "loss:  [[0.00933683]] - acc: 0.5\n",
            "Epoch 42/100\n",
            "loss:  [[0.00856873]] - acc: 0.5\n",
            "Epoch 43/100\n",
            "loss:  [[0.00786584]] - acc: 0.5\n",
            "Epoch 44/100\n",
            "loss:  [[0.00722238]] - acc: 0.5\n",
            "Epoch 45/100\n",
            "loss:  [[0.00663314]] - acc: 0.5\n",
            "Epoch 46/100\n",
            "loss:  [[0.00609334]] - acc: 0.75\n",
            "Epoch 47/100\n",
            "loss:  [[0.00559866]] - acc: 0.75\n",
            "Epoch 48/100\n",
            "loss:  [[0.00514519]] - acc: 0.75\n",
            "Epoch 49/100\n",
            "loss:  [[0.00472935]] - acc: 0.75\n",
            "Epoch 50/100\n",
            "loss:  [[0.00434791]] - acc: 0.75\n",
            "Epoch 51/100\n",
            "loss:  [[0.0039979]] - acc: 0.75\n",
            "Epoch 52/100\n",
            "loss:  [[0.00367666]] - acc: 0.75\n",
            "Epoch 53/100\n",
            "loss:  [[0.00338173]] - acc: 0.75\n",
            "Epoch 54/100\n",
            "loss:  [[0.00311089]] - acc: 0.75\n",
            "Epoch 55/100\n",
            "loss:  [[0.00286212]] - acc: 0.75\n",
            "Epoch 56/100\n",
            "loss:  [[0.00263355]] - acc: 0.75\n",
            "Epoch 57/100\n",
            "loss:  [[0.00242352]] - acc: 0.75\n",
            "Epoch 58/100\n",
            "loss:  [[0.00223046]] - acc: 0.75\n",
            "Epoch 59/100\n",
            "loss:  [[0.00205299]] - acc: 0.75\n",
            "Epoch 60/100\n",
            "loss:  [[0.0018898]] - acc: 0.75\n",
            "Epoch 61/100\n",
            "loss:  [[0.00173974]] - acc: 0.75\n",
            "Epoch 62/100\n",
            "loss:  [[0.00160171]] - acc: 0.75\n",
            "Epoch 63/100\n",
            "loss:  [[0.00147475]] - acc: 0.75\n",
            "Epoch 64/100\n",
            "loss:  [[0.00135793]] - acc: 0.75\n",
            "Epoch 65/100\n",
            "loss:  [[0.00125045]] - acc: 0.75\n",
            "Epoch 66/100\n",
            "loss:  [[0.00115154]] - acc: 0.75\n",
            "Epoch 67/100\n",
            "loss:  [[0.00106051]] - acc: 0.75\n",
            "Epoch 68/100\n",
            "loss:  [[0.00097672]] - acc: 0.75\n",
            "Epoch 69/100\n",
            "loss:  [[0.0008996]] - acc: 0.75\n",
            "Epoch 70/100\n",
            "loss:  [[0.0008286]] - acc: 0.75\n",
            "Epoch 71/100\n",
            "loss:  [[0.00076323]] - acc: 0.75\n",
            "Epoch 72/100\n",
            "loss:  [[0.00070305]] - acc: 0.75\n",
            "Epoch 73/100\n",
            "loss:  [[0.00064763]] - acc: 0.75\n",
            "Epoch 74/100\n",
            "loss:  [[0.0005966]] - acc: 0.75\n",
            "Epoch 75/100\n",
            "loss:  [[0.0005496]] - acc: 0.75\n",
            "Epoch 76/100\n",
            "loss:  [[0.00050632]] - acc: 0.75\n",
            "Epoch 77/100\n",
            "loss:  [[0.00046646]] - acc: 0.75\n",
            "Epoch 78/100\n",
            "loss:  [[0.00042975]] - acc: 0.75\n",
            "Epoch 79/100\n",
            "loss:  [[0.00039593]] - acc: 0.75\n",
            "Epoch 80/100\n",
            "loss:  [[0.00036478]] - acc: 0.75\n",
            "Epoch 81/100\n",
            "loss:  [[0.00033609]] - acc: 0.75\n",
            "Epoch 82/100\n",
            "loss:  [[0.00030966]] - acc: 0.75\n",
            "Epoch 83/100\n",
            "loss:  [[0.00028531]] - acc: 0.75\n",
            "Epoch 84/100\n",
            "loss:  [[0.00026288]] - acc: 0.75\n",
            "Epoch 85/100\n",
            "loss:  [[0.00024222]] - acc: 0.75\n",
            "Epoch 86/100\n",
            "loss:  [[0.00022319]] - acc: 0.75\n",
            "Epoch 87/100\n",
            "loss:  [[0.00020565]] - acc: 0.75\n",
            "Epoch 88/100\n",
            "loss:  [[0.00018949]] - acc: 0.75\n",
            "Epoch 89/100\n",
            "loss:  [[0.0001746]] - acc: 0.75\n",
            "Epoch 90/100\n",
            "loss:  [[0.00016089]] - acc: 0.75\n",
            "Epoch 91/100\n",
            "loss:  [[0.00014825]] - acc: 0.75\n",
            "Epoch 92/100\n",
            "loss:  [[0.00013661]] - acc: 0.75\n",
            "Epoch 93/100\n",
            "loss:  [[0.00012588]] - acc: 0.75\n",
            "Epoch 94/100\n",
            "loss:  [[0.00011599]] - acc: 0.75\n",
            "Epoch 95/100\n",
            "loss:  [[0.00010688]] - acc: 0.75\n",
            "Epoch 96/100\n",
            "loss:  [[9.84908672e-05]] - acc: 0.75\n",
            "Epoch 97/100\n",
            "loss:  [[9.0757746e-05]] - acc: 0.75\n",
            "Epoch 98/100\n",
            "loss:  [[8.36321643e-05]] - acc: 0.75\n",
            "Epoch 99/100\n",
            "loss:  [[7.70663369e-05]] - acc: 0.75\n",
            "Epoch 100/100\n",
            "loss:  [[7.10162452e-05]] - acc: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ4OlFmxmfwS"
      },
      "source": [
        "print(\"P(y=1|x) for [[0,0], [0,1], [1,0],[1,1]]\")\n",
        "print(y_hat)\n",
        "print(\"Predicted Outputs=\")\n",
        "print(DecisionBoundary(y_hat))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}